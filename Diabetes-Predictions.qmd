---
title: "MAT 434 Final Project"
author: 
  - name: Chloe Wright
    email: chloe.wright4@snhu.edu
    affiliations: 
      - name: Southern New Hampshire University
format: html
date: 3/31/2025
date-modified: today
date-format: long
theme: flatly
toc: true
code-fold: true
---


```{r}
#| message: false

library(tidyverse)
library(tidymodels)
library(kableExtra)
library(patchwork)
library(doParallel)
library(rpart)

data <- read_csv("diabetes_prediction_dataset.csv")

data <- data %>%
  mutate(diabetes = as.factor(diabetes))
  
names(data) <- janitor::make_clean_names(names(data))

set.seed(434)
split <- initial_split(data, prop = 0.8, strata = diabetes) 

train <- training(split)
test <- testing(split)

unregister <- function() {
  env <- foreach:::.foreachGlobals
  rm(list=ls(name=env), pos=env)
}

train <- train %>%
  mutate(diabetes = factor(diabetes, levels = c("0", "1")))

test <- test %>%
  mutate(diabetes = factor(diabetes, levels = c("0", "1")))

train_folds <- vfold_cv(train, v = 10)
```

## Statement of Purpose

Diabetes has become one of the leading causes of death according to "", the goal of this project is to develop a predictive model that determines whether a patient has diabetes based on their medical and demographic information given to us in this data set. Using features such as age, BMI, hypertension, heart disease, smoking history, HbA1c level, and blood glucose level, we will create a model that will find and analyze patterns within the dataset to accurately classify patients as diabetic or non-diabetic. The creation of this model aims to aid in early detection and support informed medical decision-making.

## Introduction

Diabetes is one of the most common chronic health conditions worldwide—and its growing prevalence makes early detection more important than ever. With the rise of data-driven healthcare, predictive modeling offers a powerful way to support clinicians in identifying at-risk patients before serious complications arise.

In this project, I explore the use of machine learning to predict whether a patient has diabetes based on medical and demographic data. The dataset includes features like age, gender, BMI, hypertension, heart disease, smoking history, HbA1c levels, and blood glucose levels—factors that are often considered in real-world diagnoses. By analyzing these variables, the goal is to build a classification model that can accurately distinguish between diabetic and non-diabetic patients.

The process begins with exploratory data analysis to understand the structure and trends in the data, followed by training and evaluating several machine learning models. Metrics such as accuracy, precision, and recall will help assess how well the model performs. Beyond model performance, this project aims to show how even a modest dataset, when carefully analyzed, can offer valuable insights into patient health and help lay the groundwork for smarter, more proactive care.

## Executive Summary



## Exploratory Analysis

Lets take a look at the first 6 rows of our data, and a summary of each of our variables.

```{r}
data %>% 
  head
```
```{r}
data %>% 
  summary
```
Lets take a closer look at each of our variables through plots. We will start with our numeric variables.

```{r}
train %>% 
  pivot_longer(cols = c(age, bmi, hb_a1c_level, blood_glucose_level)) %>%
  ggplot(aes(x = value)) +
  geom_histogram(bins = 30, fill = "steelblue", color = "white") +
  facet_wrap(~ name, scales = "free") +
  labs(title = "Distribution of Numeric Variables")
```
From these plots, we are able to see that our age variable tends to have a mostly normal distribution besides an outlier of 80 years old. Our blood glucose (BG) level plot shows us that a majority of the BG's are in the normal range or sightly above. A normal range of BG for a non-diabetic is between 80-120 according to "find source". I have to assume that this data-set is referring to Type 2 Diabetes due to the variables all being risk factors to Type 2. 

```{r}
train %>%
  pivot_longer(cols = c(age, bmi, hb_a1c_level, blood_glucose_level)) %>%
  ggplot(aes(x = as.factor(diabetes), y = value, fill = as.factor(diabetes))) +
  geom_boxplot() +
  facet_wrap(~ name, scales = "free") +
  labs(x = "Diabetes", y = "Value", title = "Numeric Variables by Diabetes Status")
```
These box plots are able to give us a lot of insight such as the patients with diabetes tend to be most around the range of 60 years old. The as expected, the higher the blood glucose level and Hb A1c levels are the more correlated with diabetes they are, which is accurate to the definition of diabetes. 

```{r}
ggplot(train, aes(x = age, y = hb_a1c_level, color = diabetes)) +
  geom_point(alpha = 0.5) +
  facet_wrap(~ hypertension) +
  labs(title = "HbA1c vs Age by Hypertension and Diabetes Status")
```
From this plot we are able to see that a patient with hypertension having an A1c between 5 and 7 is more likely to have diabetes than not have diabetes. We can also notice that a patient is most likely to have hypertension is they are over 20 years old. 

```{r}
train %>%
  mutate(across(c(hypertension, heart_disease, smoking_history), as.character)) %>%
  pivot_longer(cols = c(gender, hypertension, heart_disease, smoking_history)) %>%
  ggplot(aes(x = value, fill = as.factor(diabetes))) +
  geom_bar(position = "fill") +
  facet_wrap(~ name, scales = "free") +
  labs(x = "Category", y = "Proportion", fill = "Diabetes", title = "Categorical Variables by Diabetes Status")
```

Each panel in this plot displays a bar chart showing the distribution of diabetic and non-diabetic patients within each category of the variable. The use of geom_bar(position = "fill") allows for direct comparison of proportions within each group. Based on these plots we are able to see that the proportion of diabetic and non-diabetic does not vary strongly between genders, which tells us that it might not need to be a feature in our model building. However we are able to see that heart disease and hypertension do have a strong correlation with a diabetes diagnosis. Smoking history, we can now see is split up in an awkward way that makes it hard to decifer any patterns. We will mutate the smoking history variable to be more readable. 

```{r}
train <- train %>%
  mutate(smoking_history = case_when(
    smoking_history %in% c("current") ~ "current",
    smoking_history %in% c("former", "ever", "not current") ~ "former",
    smoking_history == "never" ~ "never",
    smoking_history == "no info" ~ "no info",
    TRUE ~ smoking_history 
  ))
```

Now that that variable is changed to be more readable we will check its correlation with diabetes.

```{r}
train %>%
  ggplot(aes(x = smoking_history, fill = as.factor(diabetes))) +
  geom_bar(position = "fill") +
  labs(
    x = "Smoking History",
    y = "Proportion",
    fill = "Diabetes",
    title = "Proportion of Diabetes Status by Smoking History"
  ) +
  theme_minimal()
```
Now we can see from this bar graph that there is a slight increase in diabetes diagnoses in people who have previously smoked or are current smokers. However it doesn't feel like a strong enough difference to make this a feature in our model. 

```{r}
train %>%
  group_by(diabetes) %>%
  summarise(across(c(age, bmi, hb_a1c_level, blood_glucose_level), mean, na.rm = TRUE))
```

From splitting the numeric data and finding the mean for each variable dependent on diabetes or not, we are able to see some key differences. We can see that the average age range differs about 20 years which is a large enough gap to consider it as a factor to diagnosis. Then we go to bmi, which if diabetic averages around 31.9, which according to the American Society of Metabolic and Bariatric Surgery is considered obesity, which is a major risk factor for Type 2 diabetes. This tells us that it would be a significant feature for our model. Same can be said for Hb A1c levels and blood glucose levels, there is a significant jump in A1c and blood glucose for diabetics compared to non-diabetics. 

```{r}
tree <- rpart(diabetes ~ ., data = train, method = "class")
rpart.plot::rpart.plot(tree)
```
We can see from this quick decision tree that Hb A1c is a large determinant in whether a patient has diabetes or not, with it being the root split. This tree shows us that blood glucose only really matters if Hb A1c is under a 6.7, showing that these features have a large interaction. 

Based on the exploratory data analysis done, we can determine that the best features for our first model will be age, bmi, Hb A1c, blood glucose, hypertension, and heart disease. 

## Model Construction and Assesment

### Simple Linear Regression

```{r}
log_reg_spec <- logistic_reg() %>%
  set_engine("glm") %>%
  set_mode("classification")

log_reg_rec <- recipe(diabetes ~ age + bmi + hb_a1c_level + blood_glucose_level + hypertension + heart_disease, data = train) %>%
  step_impute_median(all_numeric_predictors()) %>%
  step_impute_mode(all_nominal_predictors()) %>%
  step_dummy(all_nominal_predictors())

log_reg_wf <- workflow() %>%
  add_model(log_reg_spec) %>%
  add_recipe(log_reg_rec)

log_reg_fit <- log_reg_wf %>%
  fit(data = train)

log_reg_fit %>%
  extract_fit_engine() %>%
  tidy() 
```

We are able to interpret from this fitted engine that all of the predictors we chose are statistically significant becasue their p-values are under 0.05. Our estimates for each predictor show us precisely how significant each variable is. It shows us that each increase of Hb A1c level by 1 increases the odds of having diabetes by 2.35, and if the patient has hypertension or heart disease it increases the odds of having diabetes by about 0.79. So we can conclude from this first model that we have chosen good predictors for our model. Now we can cross validate our model and look at the performance metrics we are interested in tracking (`my_metrics`), then we'll run the *cross-validation* procedure with the `fit_resamples()` function, and collect the performance metrics using `collect_metrics()`.

```{r}
my_metrics <- metric_set(accuracy, precision, recall)

log_reg_cv_results <- log_reg_wf %>%
  fit_resamples(
    resamples = train_folds,
    metrics = my_metrics
  )

log_reg_cv_results %>%
  collect_metrics() 
```

Notice that the average recall over the ten cross-validation folds was around 99.13%. This indicates that we should expect a model of this particular form (a logistic regressor with the predictors we utilized and the feature engineering steps in our recipe) to correctly capture about 99% of patients who have diabetes. Have we over-fitted this model? Lets try a random forest model to compare. 


### Randome Forest Model
```{r}
rf_spec <- rand_forest(mtry = tune(), min_n = tune(), trees = 500) %>%
  set_engine("ranger") %>%
  set_mode("classification")

rf_recipe <- recipe(diabetes ~ age + bmi + hb_a1c_level + blood_glucose_level + hypertension + heart_disease, data = train) %>%
  step_impute_median(all_numeric_predictors()) %>%
  step_impute_mode(all_nominal_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_normalize(all_numeric_predictors())

rf_wf <- workflow() %>%
  add_model(rf_spec) %>%
  add_recipe(rf_recipe)
```

Tuning our hyperparameters with parallel processing

```{r}
n_cores <- parallel::detectCores()
cluster <- parallel::makeCluster(n_cores - 1, type = "PSOCK")
doParallel::registerDoParallel(cluster)

tictoc::tic()


rf_grid <- grid_random(
  mtry(range = c(1, 6)),  
  min_n(range = c(2, 15)),
  size = 10
)

rf_tuned <- tune_grid(
  rf_wf,
  resamples = train_folds,
  grid = rf_grid,
  metrics = metric_set(accuracy, recall, precision, roc_auc),
  control = control_grid(save_pred = TRUE)
)

tictoc::toc()

parallel::stopCluster(cluster)

rf_tuned %>%
  collect_metrics() 
```

From tuning our random forest model, we get a recall of 1.0 with the hyperparameters of 1 `mtry` and 14 `min_n`, as well as 1 `mtry` and 9 `min_n`. This can be a sign of over-fitting as well because recall measures the proportion of actual positives that our model correctly identified. A recall of 1 means our model identified all diabetic patients in the training set. Now to ensure that this is not over-fitting we want to confirm that it is not getting false positives. We can see that for each of our recalls that are 1.0, our precision means rest at around 0.97, which tells us that there are few false positives. So we can confirm that this model is performing well. 

We can compare our random forest model with hyperparameter tuning and our logistic regression model with cross validation. 

```{r}
rf_metrics <- collect_metrics(rf_tuned) %>%
  mutate(model = "Random Forest")

log_metrics <- collect_metrics(log_reg_cv_results) %>%
  mutate(model = "Logistic Regression")

all_metrics <- bind_rows(rf_metrics, log_metrics)

plot_data <- all_metrics %>%
  filter(.metric %in% c("accuracy", "recall", "precision", "roc_auc"))
  
ggplot(plot_data, aes(x = model, y = mean, fill = model)) +
  geom_col(position = position_dodge(width = 0.9)) +
  geom_errorbar(aes(ymin = mean - std_err, ymax = mean + std_err),
                width = 0.2, position = position_dodge(width = 0.9)) +
  facet_wrap(~ .metric, scales = "free_y") +
  labs(
    title = "Model Performance with Error Bars (CV Estimates)",
    y = "Mean Metric (± Standard Error)",
    x = "Model"
  ) +
  theme_minimal() +
  theme(legend.position = "none")
```

Based on this plot we are able to see that both models perform extremely well on the training data, with short error bars, meaning low variance. However, we can see that our random forest model performs just slightly better on the training data. Now we will test both models on the test data to ensure there is no over-fitting, and we can decide the best model for use from there, but first we must fit our random forest model. 

```{r}
best_rf <- select_best(rf_tuned, metric = "roc_auc")  

final_rf_wf <- finalize_workflow(rf_wf, best_rf)

rf_fit <- final_rf_wf %>% fit(data = train)
```
Now, we will save and run the predictions from each model on the testing data.

```{r}
rf_preds <- predict(rf_fit, test, type = "prob") %>%
  bind_cols(predict(rf_fit, test)) %>%
  bind_cols(test)

log_preds <- predict(log_reg_fit, test, type = "prob") %>%
  bind_cols(predict(log_reg_fit, test)) %>%
  bind_cols(test)
```


Now, we can compare each model on the testing data using the metrics accuracy, recall, precision, and roc auc.

```{r}
metrics_to_use <- metric_set(accuracy, recall, precision, roc_auc)

rf_test_metrics <- metrics_to_use(rf_preds, truth = diabetes, estimate = .pred_class, .pred_0) %>%
  mutate(model = "Random Forest")

log_test_metrics <- metrics_to_use(log_preds, truth = diabetes, estimate = .pred_class, .pred_0) %>%
  mutate(model = "Logistic Regression")

model_comparison <- bind_rows(rf_test_metrics, log_test_metrics)

model_comparison
```


Based on this we can see that our random forest model is performing better in all metrics, therefore is the best model. 


## Model Interpretation and Inference

The logistic regression model with cross-validation performs well, however it is not the preferred choice because the random forest model  This higher recall rate is crucial in scenarios where missing even a small percentage of at-risk individuals could have severe consequences. Hyperparameter tuning plays a key role in optimizing the logistic regression model, as it adjusts parameters to enhance performance and ensure that the model generalizes well to new data.



